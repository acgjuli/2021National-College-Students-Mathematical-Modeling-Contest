{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBR0OuYqqJhoq2ncNOlJUY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenyujiehome/2021National-College-Students-Mathematical-Modeling-Contest/blob/main/case2_allstep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prepare data\n",
        "\n"
      ],
      "metadata": {
        "id": "zDgP_YYL_tjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtZpCPCP_dix"
      },
      "outputs": [],
      "source": [
        "## Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "!pip install -q arize[AutoEmbeddings]\n",
        "\n",
        "# Imports from Arize\n",
        "from arize.pandas.logger import Client\n",
        "from arize.utils.types import Environments, ModelTypes, EmbeddingColumnNames, Schema\n",
        "from arize.pandas.embeddings.tabular_generators import EmbeddingGeneratorForTabularFeatures\n",
        "random.seed(314159)  # Ensure repeatabiliy\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "## Sampling\n",
        "replace = True             # Use replacement in sampling\n",
        "\n",
        "# Anomaly Creation\n",
        "anomaly_fraction    = 0.03  # Fraction of the anomalies (0.05 = 5% of the data are anomalies in window.)\n",
        "anomaly_quantile    = 0.60  # Quantile about which the anomaly is centered. (Median=0.50)\n",
        "\n",
        "anomaly_cols        = ['AveRooms', 'AveOccup' , 'Population', 'AveBedrms'] # X_cols used to create anomalies\n",
        "## Arize Setings\n",
        "\n",
        "SPACE_KEY = 'd1e0b68'\n",
        "API_KEY   = '8e035808926c673e2fa'\n",
        "# Time Window Defintiions\n",
        "\n",
        "# Useful Definitions\n",
        "now               = datetime.now()\n",
        "today             = now.replace( hour=0, minute=0, second=0, microsecond=0 )\n",
        "yesterday_end     = today - timedelta( microseconds=1 )\n",
        "\n",
        "# this_month_start  = now.replace( day=1, hour=0, minute=0, second=0, microsecond=0 )\n",
        "# last_month_end    = this_month_start - timedelta( microseconds = 1 )\n",
        "# last_month_start  = last_month_end.replace( day=1, hour=0, minute=0, second=0, microsecond=0 )\n",
        "\n",
        "# prod window\n",
        "current_prod_day = datetime.today()\n",
        "prod_start       = today - timedelta( days=20 ) # current_prod_day - timedelta( days=20 )\n",
        "prod_end         = yesterday_end\n",
        "\n",
        "print( \"production is from: \" + str( prod_start.date() ) + \" through \" + str( prod_end.date() ) )\n",
        "\n",
        "# anomaly occurs from the 24 thru 26 of the month\n",
        "anom_start     = ( prod_end - timedelta( days=6 ) ).replace( hour=0, minute=0, second=0)\n",
        "anom_end       = ( prod_end - timedelta( days=3 ) ).replace(second=0, microsecond=0, minute=0, hour=0 ) - timedelta( microseconds=1 )\n",
        "\n",
        "print( \"Anomalies is from: \" + str( anom_start.date() ) + \" through \" + str( anom_end.date() ) )\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "data = fetch_california_housing(as_frame=True).frame\n",
        "\n",
        "y_col  = 'MedHouseVal'               # Column Name of dependent variable `y`\n",
        "X_cols = data.columns.drop(y_col)    # Column Name of independent variables of `X`\n",
        "\n",
        "# data.describe()  # Describe Data\n",
        "# data.hist(bins=30, figsize=(15, 10))  # Show Histograms\n",
        "# Generate Data `train` and `prod` by sampling from `data` with replacement\n",
        "\n",
        "## Training Data\n",
        "train = data.sample( n=5000, replace=replace ).reset_index()\n",
        "train['timestamp'] = current_prod_day  # This doesn't matter it could be anytime out o\n",
        "\n",
        "## Production Data\n",
        "prod_obs_list = []    # list of observation to be appended\n",
        "day_to_set = prod_end\n",
        "\n",
        "while day_to_set >= prod_start:\n",
        "  #Randomize daily volume except for anomalies (for convenience)\n",
        "  if ( day_to_set >= anom_start ) and (day_to_set <= anom_end ):\n",
        "      daily_volume = 1000\n",
        "  else:\n",
        "    # 500-2000 productions obs / day\n",
        "    daily_volume = random.randint(500, 2000)\n",
        "\n",
        "  #Create a days worth of dataframe data as random sample from raw day\n",
        "  data_day = data.sample(n=daily_volume, replace=replace)\n",
        "  data_day['timestamp'] = day_to_set\n",
        "  prod_obs_list.append(data_day)\n",
        "\n",
        "  day_to_set = day_to_set - timedelta(days=1)  # Iterate backwards.\n",
        "\n",
        "  prod = pd.concat( prod_obs_list ).reset_index()\n",
        "## Create Anomalies\n",
        "# See settings for: anomaly_quantile, anomaly_fraction, anomaly_cols\n",
        "\n",
        "total_volume_anomaly_days = len( prod[( prod['timestamp'] >=  anom_start ) & ( prod['timestamp'] <= anom_end ) ] )\n",
        "total_anomaly_days = (anom_end - anom_start).days + 1\n",
        "\n",
        "n_anomalies_per_day = int(anomaly_fraction * (total_volume_anomaly_days/total_anomaly_days) * (1 / (1-anomaly_fraction)))\n",
        "n_anomalies =   n_anomalies_per_day * total_anomaly_days\n",
        "\n",
        "\n",
        "anomalies = pd.DataFrame()  # DataFrame to hold each anomaly\n",
        "\n",
        "# Make a DF by repeating that observation\n",
        "for i in range( n_anomalies ):\n",
        "  row = prod.sample(n=1,replace=True)\n",
        "  anomalies = pd.concat( [ anomalies, row ] )\n",
        "\n",
        "anomalies = anomalies.reset_index( drop = True )\n",
        "\n",
        "\n",
        "# Set `anomaly` flag\n",
        "train['anomaly'] = False\n",
        "prod['anomaly'] = False\n",
        "anomalies['anomaly'] = True\n",
        "# Generate Timestamp.\n",
        "anomalies['timestamp'] = pd.to_datetime(\n",
        "    pd.Series(\n",
        "      np.random.uniform( anom_start.timestamp(), anom_end.timestamp(), n_anomalies )\n",
        "    )\n",
        "    , unit='s'\n",
        "  )\n",
        "\n",
        "# Generate Anomalies\n",
        "for x in anomaly_cols:\n",
        "  quantile_for_column = data[x].quantile( anomaly_quantile )\n",
        "  print(\"Column \" + x + \" quantile \" + str( round(quantile_for_column, 3)) +\" setting to +/- 0.95/1.05 is \" + str( round(0.95*quantile_for_column,3) ) + \" \" + str( round(1.05*quantile_for_column, 3)) )\n",
        "  anomalies[x] = quantile_for_column* np.random.uniform( 0.95, 1.05, anomalies.shape[0])\n",
        "\n",
        "print( \"Anomalies created: \" + str(anomalies.shape[0]) )\n",
        "if sum(prod.anomaly) == 0:              # Ensures idempotency, this is done one time.\n",
        "  prod=pd.concat( [prod, anomalies] )\n",
        "\n",
        "prod = prod.sample(frac=1, axis=0).reset_index(drop=True)\n",
        "## (Optional) Train Model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# target = 'MedHouseVal'\n",
        "drop_col = [ 'timestamp', 'index' ]\n",
        "drop_col.append( y_col )\n",
        "\n",
        "# x_cols = train.columns.drop( drop_col )\n",
        "\n",
        "y = train[ y_col ]\n",
        "X = train[ X_cols ]\n",
        "\n",
        "# Train Model\n",
        "regr = RandomForestRegressor(max_depth=5, random_state=0)\n",
        "regr.fit(X, y)\n",
        "pred_col = 'prediction'\n",
        "\n",
        "train[ pred_col ] = regr.predict( train[ X_cols ] )\n",
        "prod[ pred_col ] = regr.predict( prod[ X_cols ] )\n",
        "# Install 'arize' package\n",
        "\n",
        "\n",
        "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generate llama embedding"
      ],
      "metadata": {
        "id": "TFTSRd_JAU0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# 预先加载的模型和分词器\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=api)\n",
        "model = AutoModel.from_pretrained(model_name, token=api)\n",
        "# 加载DataFrame\n",
        "df = prod\n",
        "\n",
        "# 确定列名\n",
        "column_names = [f'vec_{i}' for i in range(4096)]  # 假设嵌入的长度是4096\n",
        "\n",
        "# 确定是否有可用的GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 将模型加载到GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# 确定从哪一行开始处理\n",
        "start_row = 0\n",
        "output_file = \"embeddings.csv\"\n",
        "\n",
        "# 如果文件已存在，计算已处理的行数\n",
        "try:\n",
        "    with open(output_file, \"r\") as f:\n",
        "        start_row = len(f.readlines()) - 1  # 减去标题行\n",
        "    append_write = 'a'  # 追加模式\n",
        "except FileNotFoundError:\n",
        "    append_write = 'w'  # 写入模式\n",
        "\n",
        "# 开始处理\n",
        "with open(output_file, append_write) as f:\n",
        "    if append_write == 'w':\n",
        "        # 写入列名\n",
        "        f.write(\",\".join(column_names) + \"\\n\")\n",
        "\n",
        "    # 使用tqdm显示进度条\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0], initial=start_row):\n",
        "        if index < start_row:\n",
        "            continue  # 跳过已处理的行\n",
        "\n",
        "        prompt = \"\".join([f\"the {col} is {row[col]}.\" for col in df.columns])\n",
        "        tokens = tokenizer(prompt, return_tensors='pt')\n",
        "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**tokens)\n",
        "\n",
        "        embeddings = torch.mean(outputs.last_hidden_state, 1)\n",
        "        embeddings_np = embeddings.cpu().numpy()[0]\n",
        "        embeddings_str = \",\".join(map(str, embeddings_np))\n",
        "        f.write(embeddings_str + \"\\n\")\n",
        "\n",
        "        torch.cuda.empty_cache()  # 清理GPU缓存\n"
      ],
      "metadata": {
        "id": "R0Ra_Ls-A69e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"embeddings.csv\")\n",
        "# 假设 'df' 是您原始的 DataFrame\n",
        "list_of_vectors = df.values.tolist()\n",
        "prod['tabular_vector'] = list_of_vectors\n",
        "df.to_pickle(\"prod.pkl\")"
      ],
      "metadata": {
        "id": "waVm5yx9BLBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# send to arize"
      ],
      "metadata": {
        "id": "BA38ErCLCrJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs_in_anomaly_window = len(            # Observations in anomaly window\n",
        "    prod[\n",
        "         ( prod['timestamp'] >= anom_start) & ( prod['timestamp'] <= anom_end )\n",
        "        ]\n",
        ")\n",
        "\n",
        "actual_anomalies = sum(prod.anomaly)    # Number of anomalies\n",
        "\n",
        "final_anomaly_fraction = actual_anomalies/obs_in_anomaly_window\n",
        "\n",
        "print( \"days with anomalies: \" + str( obs_in_anomaly_window ) )\n",
        "print( \"actual anomalies: \" + str( actual_anomalies) )\n",
        "print( \"fraction anolalies: \" + str( np.round(final_anomaly_fraction, 4) ) )\n",
        "\n",
        "# Lets create a model ID based on time for this run - as a string\n",
        "#import datetime\n",
        "\n",
        "model_id = \"anomaly-\" + \\\n",
        "  datetime.now().strftime( \"%Y_%m_%d_%H:%m\" ) + \\\n",
        "  \"-frac_anom_\" + \"{:.2f}\".format(round(final_anomaly_fraction, 2)) + \\\n",
        "  \"-quant_\" + str(anomaly_quantile) + \\\n",
        "  \"-anom_columns_\" + str(len(anomaly_cols))\n",
        "\n",
        "print( \"model name is: \\n  \" + model_id )\n",
        "# Model ID\n",
        "\n",
        "model_version = \"1.3\"\n",
        "model_type = ModelTypes.REGRESSION\n",
        "\n",
        "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
        "    raise ValueError(\"❌ NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
        "else:\n",
        "    print(\"✅ Import and Setup Arize Client Done! Now we can start using Arize!\")\n",
        "    embedding_features = {\n",
        "    # Dictionary keys will be name of embedding feature in the app\n",
        "    \"tabular embedding\": EmbeddingColumnNames(\n",
        "        vector_column_name = \"tabular_vector\",\n",
        "        #  data_column_name=\"prompt\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Define a Schema() object for Arize to pick up data from the correct columns for logging\n",
        "schema = Schema(\n",
        "    timestamp_column_name           = 'timestamp',\n",
        "    prediction_id_column_name       = 'index',\n",
        "    prediction_label_column_name    = pred_col ,\n",
        "    actual_label_column_name        = y_col,\n",
        "    feature_column_names            = list(X_cols) + ['index'],\n",
        "    embedding_feature_column_names  = embedding_features,\n",
        "    tag_column_names                = [ 'anomaly' ]\n",
        ")\n",
        "# Logging Production DataFrame\n",
        "prod = prod.reset_index( drop=True)\n",
        "response = arize_client.log(\n",
        "    dataframe     = prod,\n",
        "    model_id      = model_id,\n",
        "    model_version = model_version,\n",
        "    model_type    = model_type,\n",
        "    environment   = Environments.PRODUCTION,\n",
        "    schema        = schema,\n",
        ")\n",
        "\n",
        "print( \"prod data with \" + str(prod.shape[0]) + \" rows sent at: \" + str( datetime.now() ) )\n",
        "print( \"Anomalies are found between: \" + str(anom_start.date()) + \" thru \" + str(anom_end.date()) )"
      ],
      "metadata": {
        "id": "m0YURadoCvLi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}